# -*- coding: utf-8 -*-
"""roberta.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14x3pZkn66qpfjKB4MypUobxFSCPxCtyV
"""

import nltk
from collections import Counter
import collections
import pandas as pd
import json
from transformers import RobertaTokenizer, RobertaForQuestionAnswering
from transformers import pipeline
!pip install transformers


# Load model & tokenizer
roberta_model = RobertaForQuestionAnswering.from_pretrained('roberta-base')
roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')

# Get predictions
nlp = pipeline('question-answering', model=roberta_model,
               tokenizer=roberta_tokenizer)

result = nlp({
    'question': 'How many people live in Berlin?',
    'context': 'Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.'
})

print(result)

path = "/content/drive/MyDrive/Colab Notebooks/data/qar-squad.json"


def getDF(path):
    i = 0
    df = {}
    # read json file in python object
    with open(path, 'r') as fp:
        for line in fp:
            # load json lines and store in dictionary
            df[i] = json.loads(line)
            i += 1
    # convert dict to dataframe
    return pd.DataFrame.from_dict(df, orient='index')


amzn = getDF(path)

# contexts = []
# for i in range(len(df['context'])):
#   contexts.append(df['context'][i])

# contexts[0]

qa_data = amzn

df = pd.DataFrame()
contexts = qa_data['context']
versions = []
data = []
for i in range(len(contexts)):
    lst = []
    paragraphs = []
    samp_dict = {}
    answers = []
    samp = qa_data['qas'][i]
    sample = samp[0]
    answer = sample['answers_sentence_bleu2'][0]
    answers.append(answer)
    samp_dict['answers'] = answers
    samp_dict['id'] = '2t523njt3t'
    samp_dict['question'] = sample['question']
    lst.append(samp_dict)
    context = qa_data['context'][i]
    paradict = {}
    paradict['context'] = context
    paradict['qas'] = lst
    paragraphs.append(paradict)
    para = {}
    para['paragraphs'] = paragraphs
    para['title'] = "Review"
    versions.append("1.1")
    data.append(para)
pd.Series(data)
pd.Series(versions)
df['version'] = versions
df['data'] = data
df.head()

qa_data = df.loc[0:150, :]

qa_data

lookup = 'abcdefghijklmnopqrstuvwxyz1234567890?.,'
# check for valid characters


def in_white_list(_word):
    valid_word = False
    for char in _word:
        if char in lookup:
            valid_word = True
            break

    if valid_word is False:
        return False

    return True

# extract paragraphs,question and answers from SQuAD_data


def get_amzn_data(qa_data, max_data_count, max_context_seq_length, max_question_seq_length, max_target_seq_length):
    data = list()
    for instance in qa_data['data']:
        for paragraph in instance['paragraphs']:
            context = paragraph['context']
            context_wid_list = [w.lower() for w in nltk.word_tokenize(
                context) if in_white_list(w)]
            if len(context_wid_list) > max_context_seq_length:
                continue
            qas = paragraph['qas']
            for qas_instance in qas:
                question = qas_instance['question']
                question_wid_list = [w.lower() for w in nltk.word_tokenize(
                    question) if in_white_list(w)]
                if len(question_wid_list) > max_question_seq_length:
                    continue
                answers = qas_instance['answers']
                for answer in answers:
                    ans = answer['text']
                    answer_wid_list = [
                        w.lower() for w in nltk.word_tokenize(ans) if in_white_list(w)]
                    if len(answer_wid_list) > max_target_seq_length:
                        continue
                    if len(data) < max_data_count:
                        data.append((context, question, ans))

            if len(data) >= max_data_count:
                break

            break
    return data


nltk.download("punkt")

max_data_count = 100000
max_context_seq_length = 512
max_question_seq_length = 60
max_target_seq_length = 50
data = get_amzn_data(qa_data, max_data_count, max_context_seq_length,
                     max_question_seq_length, max_target_seq_length)

type(data)

len(data)

type(data[0])

data[0]

question = []
answer = []
context = []
for d in data:
    context.append(d[0])
    question.append(d[1])
    answer.append(d[2])

type(question)

question[0]

type(question[0])

listt = []
for i in range(len(question)):
    d = {}
    d['question'] = question[i]
    d['context'] = context[i]
    listt.append(d)

listt

results = []
for i in range(len(listt)):
    result = nlp(listt[i])
    results.append(result)
    print(result)


def normalize_text(s):
    """Removing articles and punctuation, and standardizing whitespace are all typical text processing steps."""
    import string
    import re

    def remove_articles(text):
        regex = re.compile(r"\b(a|an|the)\b", re.UNICODE)
        return re.sub(regex, " ", text)

    def white_space_fix(text):
        return " ".join(text.split())

    def remove_punc(text):
        exclude = set(string.punctuation)
        return "".join(ch for ch in text if ch not in exclude)

    def lower(text):
        return text.lower()

    return white_space_fix(remove_articles(remove_punc(lower(s))))


def compute_f1(prediction, truth):
    pred_tokens = normalize_text(prediction).split()
    truth_tokens = normalize_text(truth).split()

    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise
    if len(pred_tokens) == 0 or len(truth_tokens) == 0:
        return int(pred_tokens == truth_tokens)

    common_tokens = set(pred_tokens) & set(truth_tokens)

    # if there are no common tokens then f1 = 0
    if len(common_tokens) == 0:
        return 0

    prec = len(common_tokens) / len(pred_tokens)
    rec = len(common_tokens) / len(truth_tokens)

    return 2 * (prec * rec) / (prec + rec)


def compute_exact_match(prediction, truth):
    return int(normalize_text(prediction) == normalize_text(truth))


pred_ans = []
for i in range(len(results)):
    pred_ans.append(results[i]['answer'])

actual_ans = answer


def f1(actual_ans, pred_ans):
    maxx = 0
    for i in range(len(actual_ans)):
        val = compute_f1(actual_ans[i], pred_ans[i])
        maxx = max(maxx, val)
    return maxx


f1(actual_ans, pred_ans)


def question_answer(question, reviews_text):

    return "RoBERTa Answer"
